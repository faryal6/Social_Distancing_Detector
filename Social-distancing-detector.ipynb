{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from configs import config\n",
    "from configs.detection import detect_people\n",
    "from scipy.spatial import distance as dist \n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89111ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--video-path\", type=str, default=\"pedestrians.mp4\", help=\"path to (optional) input video file\")\n",
    "ap.add_argument(\"-o\", \"--video-output-path\", type=str, default=\"output.avi\", help=\"path to (optional) output video file\")\n",
    "ap.add_argument(\"-d\", \"--display\", type=int, default=1, help=\"whether or not output frame should be displayed\")\n",
    "args, unknown = ap.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the COCO class labels the YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([config.MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.cfg\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe294e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is to be used or not\n",
    "if config.USE_GPU:\n",
    "    # set CUDA s the preferable backend and target\n",
    "    print(\"[INFO] setting preferable backend and target to CUDA...\")\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b665bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine only the \"output\" layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.video_path:\n",
    "# initialize the video stream and pointer to output video file\n",
    "    print(\"[INFO] accessing video stream...\")\n",
    "# open input video if available else webcam stream\n",
    "    vs = cv2.VideoCapture(args.video_path if args.video_path else 0)\n",
    "    writer = None\n",
    "\n",
    "    # loop over the frames from the video stream\n",
    "    while True:\n",
    "        # read the next frame from the input video\n",
    "        (grabbed, frame) = vs.read()\n",
    "\n",
    "        # if the frame was not grabbed, then that's the end fo the stream \n",
    "        if not grabbed:\n",
    "            break\n",
    "\n",
    "        # resize the frame and then detect people (only people) in it\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "        results = detect_people(frame, net, ln, personIdx=LABELS.index(\"person\"))\n",
    "\n",
    "        # initialize the set of indexes that violate the minimum social distance\n",
    "        violate = set()\n",
    "\n",
    "        # ensure there are at least two people detections (required in order to compute the\n",
    "        # the pairwise distance maps)\n",
    "        if len(results) >= 2:\n",
    "            # extract all centroids from the results and compute the Euclidean distances\n",
    "            # between all pairs of the centroids\n",
    "            centroids = np.array([r[2] for r in results])\n",
    "            D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "            # loop over the upper triangular of the distance matrix\n",
    "            for i in range(0, D.shape[0]):\n",
    "                for j in range(i+1, D.shape[1]):\n",
    "                    # check to see if the distance between any two centroid pairs is less\n",
    "                    # than the configured number of pixels\n",
    "                    if D[i, j] < config.MIN_DISTANCE:\n",
    "                        # update the violation set with the indexes of the centroid pairs\n",
    "                        violate.add(i)\n",
    "                        violate.add(j)\n",
    "\n",
    "        # loop over the results\n",
    "        for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "            # extract teh bounding box and centroid coordinates, then initialize the color of the annotation\n",
    "            (startX, startY, endX, endY) = bbox\n",
    "            (cX, cY) = centroid\n",
    "            color = (0, 255, 0)\n",
    "\n",
    "            # if the index pair exists within the violation set, then update the color\n",
    "            if i in violate:\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            # draw (1) a bounding box around the person and (2) the centroid coordinates of the person\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "            cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "\n",
    "        # draw the total number of social distancing violations on the output frame\n",
    "        text = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "        cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "\n",
    "        # check to see if the output frame should be displayed to the screen\n",
    "        if args.display > 0:\n",
    "            # show the output frame\n",
    "            cv2.imshow(\"Output\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the 'q' key is pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # if  the video writer has not been  as none\n",
    "        if writer is None:\n",
    "            # initialize the video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "            writer = cv2.VideoWriter(args.video_output_path, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "        # if the video writer is not None, write the frame to the output video file\n",
    "        if writer is not None:\n",
    "            print(\"[INFO] writing stream to output\")\n",
    "            writer.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a027932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24df3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3f2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
